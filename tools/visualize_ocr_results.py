#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
tools/visualize_ocr_results.py - OCRãƒ†ã‚¹ãƒˆçµæœã®å¯è¦–åŒ– + ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ç•°å¸¸æ¤œçŸ¥

ä½¿ã„æ–¹:
  python tools/visualize_ocr_results.py --input tests/outputs/ocr_dataset_eval.json
"""

import argparse
import json
import pathlib
import sys
from datetime import datetime
from collections import defaultdict, Counter

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np
import scipy.stats as stats
from sklearn.covariance import LedoitWolf

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è·é›¢å˜ä½ã®å›ºå®šï¼ˆDÂ²ã§çµ±ä¸€ï¼‰
# æ³¨æ„: ä»¥ä¸‹ã®é–¾å€¤ã¯æ¨™æº–åŒ–ãªã—ã®æ­£ã—ã„ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ç”¨ã«å†èª¿æ•´ãŒå¿…è¦
THEORETICAL_THRESHOLD_D2 = 15.51  # Ï‡Â²(df=8) 95%ç‚¹ï¼ˆç†è«–å€¤ï¼‰
EMPIRICAL_THRESHOLD_D2 = 26.0     # å®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãé–¾å€¤ï¼ˆè¦å†æ¨å®šï¼‰
WARNING_THRESHOLD_D2 = 18.0       # æº–ç•°å¸¸é–¾å€¤ï¼ˆè¦å†æ¨å®šï¼‰


def load_results(json_path: pathlib.Path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚¹ãƒˆçµæœã‚’èª­ã¿è¾¼ã‚€"""
    with json_path.open(encoding='utf-8') as f:
        return json.load(f)


def extract_features(results):
    """ãƒ†ã‚¹ãƒˆçµæœã‹ã‚‰8æ¬¡å…ƒç‰¹å¾´é‡ã‚’æŠ½å‡º"""
    features = []
    for r in results:
        # 8æ¬¡å…ƒç‰¹å¾´é‡ã®æ§‹æˆ
        feature_vector = [
            r.get('cer', 0.0),                    # 1. CER
            r.get('latency_ms', 0.0),             # 2. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
            len(r.get('detected_text', '')),       # 3. æ¤œå‡ºæ–‡å­—æ•°
            len(r.get('ground_truth', '')),        # 4. æ­£è§£æ–‡å­—æ•°
            r.get('confidence', 1.0),              # 5. ä¿¡é ¼åº¦
            len(r.get('dt_boxes', [])),            # 6. æ¤œå‡ºãƒœãƒƒã‚¯ã‚¹æ•°
            r.get('ocr_ms', 0.0),                  # 7. OCRå‡¦ç†æ™‚é–“
            r.get('post_ms', 0.0),                 # 8. å¾Œå‡¦ç†æ™‚é–“
        ]
        features.append(feature_vector)
    return np.array(features)


def compute_mahalanobis_distances(features, cov_estimator='ledoit_wolf'):
    """ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ï¼ˆDÂ²ï¼‰ã‚’è¨ˆç®— - Leave-One-Outæ–¹å¼
    
    æ­£ã—ã„ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®å®šç¾©:
    DÂ² = (x - Î¼)áµ€ Î£â»Â¹ (x - Î¼)
    
    ã“ã“ã§:
    - x: ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«
    - Î¼: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
    - Î£: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å…±åˆ†æ•£è¡Œåˆ—
    
    æ³¨æ„: æ¨™æº–åŒ–ã¯ä¸è¦ã€‚ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢è‡ªä½“ãŒã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ã€‚
    """
    n_samples, n_features = features.shape
    distances_d2 = []
    
    for i in range(n_samples):
        # LOO: iç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’é™¤ã„ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿
        train_mask = np.ones(n_samples, dtype=bool)
        train_mask[i] = False
        X_train = features[train_mask]  # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆæ¨™æº–åŒ–ãªã—ï¼‰
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
        center = X_train.mean(axis=0)
        
        # å…±åˆ†æ•£è¡Œåˆ—æ¨å®š
        if cov_estimator == 'ledoit_wolf':
            # Ledoit-Wolfæ¨å®šï¼ˆå°‘æ•°æ¨™æœ¬ã«é©ç”¨ï¼‰
            cov_est = LedoitWolf()
            cov_est.fit(X_train)
            cov_matrix = cov_est.covariance_
        else:
            # çµŒé¨“å…±åˆ†æ•£
            cov_matrix = np.cov(X_train.T)
        
        # å…±åˆ†æ•£è¡Œåˆ—ã®é€†è¡Œåˆ—
        try:
            cov_inv = np.linalg.inv(cov_matrix)
        except np.linalg.LinAlgError:
            # é€†è¡Œåˆ—ãŒè¨ˆç®—ã§ããªã„å ´åˆã¯æ“¬ä¼¼é€†è¡Œåˆ—ã‚’ä½¿ç”¨
            cov_inv = np.linalg.pinv(cov_matrix)
        
        # ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ï¼ˆiç•ªç›®ï¼‰ã®ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢DÂ²ã‚’è¨ˆç®—
        test_sample = features[i]  # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆæ¨™æº–åŒ–ãªã—ï¼‰
        diff = test_sample - center
        d_squared = np.dot(np.dot(diff, cov_inv), diff)
        distances_d2.append(d_squared)
    
    return np.array(distances_d2), cov_estimator


def classify_anomaly_level(distance_d2):
    """ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢DÂ²ã«åŸºã¥ãç•°å¸¸ãƒ¬ãƒ™ãƒ«åˆ†é¡"""
    if distance_d2 > EMPIRICAL_THRESHOLD_D2:
        return 'strong_anomaly', 'degrade'
    elif distance_d2 > WARNING_THRESHOLD_D2:
        return 'weak_anomaly', 'warn'
    else:
        return 'normal', 'normal'


def create_mahalanobis_analysis(results, distances_d2, output_dir: pathlib.Path):
    """ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®åˆ†æãƒ»å¯è¦–åŒ–"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. è·é›¢ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + é–¾å€¤
    ax1.hist(distances_d2, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.axvline(THEORETICAL_THRESHOLD_D2, color='red', linestyle='--', linewidth=2, 
                label=f'ç†è«–: Ï‡Â²95% (={THEORETICAL_THRESHOLD_D2:.1f})')
    ax1.axvline(EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2,
                label=f'çµŒé¨“: 95% (={EMPIRICAL_THRESHOLD_D2:.1f})')
    ax1.axvline(WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2,
                label=f'æº–ç•°å¸¸ (={WARNING_THRESHOLD_D2:.1f})')
    ax1.set_xlabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax1.set_ylabel('é »åº¦', fontsize=10, fontweight='bold')
    ax1.set_title('LOOè·é›¢åˆ†å¸ƒã¨é–¾å€¤', fontsize=12, fontweight='bold')
    ax1.legend(fontsize=9)
    ax1.grid(alpha=0.3)
    
    # 2. QQãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ­£è¦æ€§ãƒã‚§ãƒƒã‚¯ï¼‰
    stats.probplot(distances_d2, dist="chi2", sparams=(8,), plot=ax2)
    ax2.set_title('QQãƒ—ãƒ­ãƒƒãƒˆ (Ï‡Â²åˆ†å¸ƒ, df=8)', fontsize=12, fontweight='bold')
    ax2.grid(alpha=0.3)
    
    # 3. ã‚¿ã‚°åˆ¥è·é›¢
    tag_distances = defaultdict(list)
    for r, d in zip(results, distances_d2):
        tags = r['tags'].split('-')
        for tag in tags:
            tag_distances[tag].append(d)
    
    # ã‚¿ã‚°ã‚’ã‚½ãƒ¼ãƒˆ
    sorted_tags = sorted(tag_distances.items(), key=lambda x: np.median(x[1]))
    tags = [t[0] for t in sorted_tags]
    data = [t[1] for t in sorted_tags]
    
    bp = ax3.boxplot(data, tick_labels=tags, patch_artist=True, notch=True)
    for patch in bp['boxes']:
        patch.set_facecolor('lightcoral')
        patch.set_alpha(0.7)
    
    ax3.axhline(EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2)
    ax3.axhline(WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2)
    ax3.set_ylabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax3.set_title('ã‚¿ã‚°åˆ¥ç•°å¸¸åº¦åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(alpha=0.3)
    
    # 4. è·é›¢ vs CERæ•£å¸ƒå›³
    cers = [r['cer'] for r in results]
    ids = [r['id'] for r in results]
    
    # ç•°å¸¸ãƒ¬ãƒ™ãƒ«åˆ¥ã®è‰²åˆ†ã‘
    colors = []
    for d in distances_d2:
        level, _ = classify_anomaly_level(d)
        if level == 'strong_anomaly':
            colors.append('#e74c3c')  # èµ¤
        elif level == 'weak_anomaly':
            colors.append('#f39c12')  # ã‚ªãƒ¬ãƒ³ã‚¸
        else:
            colors.append('#2ecc71')  # ç·‘
    
    scatter = ax4.scatter(distances_d2, cers, c=colors, s=100, alpha=0.7, edgecolors='black')
    
    # IDãƒ©ãƒ™ãƒ«ï¼ˆç•°å¸¸ã®ã¿ï¼‰
    for i, (d, cer, id_) in enumerate(zip(distances_d2, cers, ids)):
        if d > WARNING_THRESHOLD_D2:
            ax4.annotate(id_, (d, cer), fontsize=8, ha='center', va='bottom')
    
    ax4.axvline(EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2)
    ax4.axvline(WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2)
    ax4.set_xlabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax4.set_ylabel('CER', fontsize=10, fontweight='bold')
    ax4.set_title('ç•°å¸¸åº¦ vs CER', fontsize=12, fontweight='bold')
    ax4.grid(alpha=0.3)
    
    # å‡¡ä¾‹
    normal_patch = mpatches.Patch(color='#2ecc71', alpha=0.7, label='é€šå¸¸é‹è»¢')
    warn_patch = mpatches.Patch(color='#f39c12', alpha=0.7, label='æº–ç•°å¸¸')
    strong_patch = mpatches.Patch(color='#e74c3c', alpha=0.7, label='å¼·ã„ç•°å¸¸')
    ax4.legend(handles=[normal_patch, warn_patch, strong_patch], fontsize=9)
    
    plt.suptitle('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥åˆ†æ', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    output_path = output_dir / 'mahalanobis_analysis.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ãƒãƒãƒ©ãƒãƒ“ã‚¹åˆ†æä¿å­˜: {output_path}")
    plt.close()


def create_operational_log(results, distances_d2, output_dir: pathlib.Path, cov_version='LedoitWolf'):
    """é‹ç”¨ãƒ­ã‚°ï¼ˆJSON Linesï¼‰ã®å‡ºåŠ›"""
    log_path = output_dir / f'ocr_operational_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.jsonl'
    
    with open(log_path, 'w', encoding='utf-8') as f:
        for r, distance_d2 in zip(results, distances_d2):
            level, decision = classify_anomaly_level(distance_d2)
            
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'image_id': r['id'],
                'tag': r['tags'],
                
                # ç‰¹å¾´é‡ï¼ˆé‹ç”¨ç›£è¦–ç”¨ï¼‰
                'features_raw': {
                    'cer': r.get('cer', 0.0),
                    'latency_ms': r.get('latency_ms', 0.0),
                    'detected_chars': len(r.get('detected_text', '')),
                    'ground_truth_chars': len(r.get('ground_truth', '')),
                    'confidence': r.get('confidence', 1.0),
                    'dt_boxes_count': len(r.get('dt_boxes', [])),
                    'ocr_ms': r.get('ocr_ms', 0.0),
                    'post_ms': r.get('post_ms', 0.0),
                },
                
                # ç•°å¸¸æ¤œçŸ¥
                'anomaly_detection': {
                    'mahal_distance_d2': float(distance_d2),
                    'cov_version': cov_version,
                    'thresholds': {
                        'theory_d2': THEORETICAL_THRESHOLD_D2,
                        'empirical_d2': EMPIRICAL_THRESHOLD_D2,
                        'warning_d2': WARNING_THRESHOLD_D2
                    },
                    'level': level,
                    'decision': decision,
                    'rationale': f'DÂ²={distance_d2:.2f} vs é–¾å€¤={EMPIRICAL_THRESHOLD_D2}'
                },
                
                # å“è³ªæŒ‡æ¨™
                'quality_metrics': {
                    'cer': r.get('cer', 0.0),
                    'levenshtein': r.get('levenshtein_distance', 0),
                    'rule_used': 'empirical_95pct',
                    'quality_ok': r.get('quality_ok', False)
                },
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
                'performance': {
                    'proc_total_ms': r.get('latency_ms', 0.0),
                    'ocr_ms': r.get('ocr_ms', 0.0),
                    'post_ms': r.get('post_ms', 0.0),
                    'engine': r.get('engine', 'unknown')
                }
            }
            
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    print(f"âœ… é‹ç”¨ãƒ­ã‚°å‡ºåŠ›: {log_path}")
    return log_path


def create_cer_bar_chart(results, distances_d2, output_dir: pathlib.Path, threshold: float = 0.30):
    """CERã®æ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆï¼ˆç•°å¸¸æ¤œçŸ¥çµ±åˆç‰ˆï¼‰"""
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), height_ratios=[2, 1])
    
    ids = [r['id'] for r in results]
    cers = [r['cer'] for r in results]
    tags = [r['tags'] for r in results]
    quality_ok = [r['quality_ok'] for r in results]
    
    # ç•°å¸¸ãƒ¬ãƒ™ãƒ«åˆ¥ã®è‰²åˆ†ã‘
    colors = []
    for d in distances_d2:
        level, _ = classify_anomaly_level(d)
        if level == 'strong_anomaly':
            colors.append('#e74c3c')  # èµ¤ï¼šå¼·ã„ç•°å¸¸
        elif level == 'weak_anomaly':
            colors.append('#f39c12')  # ã‚ªãƒ¬ãƒ³ã‚¸ï¼šæº–ç•°å¸¸
        else:
            colors.append('#2ecc71')  # ç·‘ï¼šé€šå¸¸
    
    # ä¸Šæ®µï¼šCERæ£’ã‚°ãƒ©ãƒ•
    bars = ax1.bar(ids, cers, color=colors, alpha=0.7, edgecolor='black', linewidth=1.2)
    ax1.axhline(y=threshold, color='purple', linestyle='--', linewidth=2, label=f'CERé–¾å€¤={threshold}')
    
    ax1.set_xlabel('ç”»åƒID', fontsize=12, fontweight='bold')
    ax1.set_ylabel('CER (Character Error Rate)', fontsize=12, fontweight='bold')
    ax1.set_title('OCRç²¾åº¦è©•ä¾¡çµæœï¼ˆç•°å¸¸æ¤œçŸ¥çµ±åˆç‰ˆï¼‰- PaddleOCR 2.7.0.3', fontsize=14, fontweight='bold')
    ax1.set_ylim(0, 1.0)
    ax1.grid(axis='y', alpha=0.3, linestyle=':', linewidth=0.8)
    
    # ã‚¿ã‚°ã‚’Xè»¸ãƒ©ãƒ™ãƒ«ã«è¿½åŠ 
    ax1.set_xticks(range(len(ids)))
    ax1.set_xticklabels([f"{id_}\n({tag})" for id_, tag in zip(ids, tags)], 
                        rotation=45, ha='right', fontsize=9)
    
    # CERå€¤ã‚’æ£’ã®ä¸Šã«è¡¨ç¤º
    for bar, cer in zip(bars, cers):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{cer:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    # ä¸‹æ®µï¼šãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢DÂ²
    bars2 = ax2.bar(ids, distances_d2, color=colors, alpha=0.7, edgecolor='black', linewidth=1.2)
    ax2.axhline(y=EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2, 
                label=f'çµŒé¨“95% (={EMPIRICAL_THRESHOLD_D2:.1f})')
    ax2.axhline(y=WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2,
                label=f'æº–ç•°å¸¸ (={WARNING_THRESHOLD_D2:.1f})')
    ax2.axhline(y=THEORETICAL_THRESHOLD_D2, color='red', linestyle='--', linewidth=1,
                label=f'ç†è«–95% (={THEORETICAL_THRESHOLD_D2:.1f})')
    
    ax2.set_xlabel('ç”»åƒID', fontsize=12, fontweight='bold')
    ax2.set_ylabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=12, fontweight='bold')
    ax2.set_title('ç•°å¸¸æ¤œçŸ¥ã‚¹ã‚³ã‚¢ï¼ˆDÂ²ãƒ™ãƒ¼ã‚¹ï¼‰', fontsize=12, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3, linestyle=':', linewidth=0.8)
    ax2.legend(fontsize=9, loc='upper left')
    
    # DÂ²å€¤ã‚’æ£’ã®ä¸Šã«è¡¨ç¤º
    for bar, d in zip(bars2, distances_d2):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{d:.1f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    # å‡¡ä¾‹ï¼ˆä¸Šæ®µï¼‰
    normal_patch = mpatches.Patch(color='#2ecc71', alpha=0.7, label='é€šå¸¸é‹è»¢')
    warn_patch = mpatches.Patch(color='#f39c12', alpha=0.7, label='æº–ç•°å¸¸ï¼ˆè¦è¦³å¯Ÿï¼‰')
    strong_patch = mpatches.Patch(color='#e74c3c', alpha=0.7, label='å¼·ã„ç•°å¸¸ï¼ˆãƒ‡ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰')
    cer_line = ax1.get_lines()[0]
    ax1.legend(handles=[normal_patch, warn_patch, strong_patch, cer_line], 
              loc='upper left', fontsize=10)
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¿½åŠ 
    passed = sum(quality_ok)
    total = len(results)
    pass_rate = (passed / total) * 100
    avg_cer = np.mean(cers)
    anomaly_count = sum(1 for d in distances_d2 if d > WARNING_THRESHOLD_D2)
    
    stats_text = (f'åˆæ ¼ç‡: {passed}/{total} ({pass_rate:.1f}%)\n'
                  f'å¹³å‡CER: {avg_cer:.3f}\n'
                  f'ç•°å¸¸æ¤œçŸ¥: {anomaly_count}/{total}ä»¶')
    ax1.text(0.98, 0.98, stats_text, transform=ax1.transAxes,
            fontsize=11, verticalalignment='top', horizontalalignment='right',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    output_path = output_dir / 'ocr_integrated_analysis.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… çµ±åˆåˆ†æä¿å­˜: {output_path}")
    plt.close()


def create_engine_comparison(results, output_dir: pathlib.Path):
    """ã‚¨ãƒ³ã‚¸ãƒ³åˆ¥ã®ä½¿ç”¨çŠ¶æ³ã‚’å††ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º"""
    from collections import Counter
    
    engines = [r['engine'] for r in results]
    engine_counts = Counter(engines)
    
    fig, ax = plt.subplots(figsize=(8, 8))
    
    labels = list(engine_counts.keys())
    sizes = list(engine_counts.values())
    colors = ['#3498db', '#e67e22', '#95a5a6']
    
    wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                                        startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
    
    ax.set_title('ä½¿ç”¨ã‚¨ãƒ³ã‚¸ãƒ³åˆ†å¸ƒ', fontsize=14, fontweight='bold', pad=20)
    
    # å‡¡ä¾‹
    ax.legend(wedges, [f'{label}: {count}ä»¶' for label, count in engine_counts.items()],
              loc='upper left', bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10)
    
    plt.tight_layout()
    output_path = output_dir / 'ocr_engine_distribution.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å††ã‚°ãƒ©ãƒ•ä¿å­˜: {output_path}")
    plt.close()


def create_tag_analysis(results, distances_d2, output_dir: pathlib.Path, threshold: float = 0.30):
    """ã‚¿ã‚°åˆ¥ã®CERåˆ†å¸ƒï¼‹ç•°å¸¸æ¤œçŸ¥åˆ†æ"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))
    
    # ã‚¿ã‚°åˆ¥ãƒ‡ãƒ¼ã‚¿æ•´ç†
    tag_cers = defaultdict(list)
    tag_distances = defaultdict(list)
    tag_samples = defaultdict(list)
    
    for i, r in enumerate(results):
        tags = r['tags'].split('-')
        for tag in tags:
            tag_cers[tag].append(r['cer'])
            tag_distances[tag].append(distances_d2[i])
            tag_samples[tag].append(r['id'])
    
    # ã‚¿ã‚°ã‚’CERã®ä¸­å¤®å€¤ã§ã‚½ãƒ¼ãƒˆ
    sorted_tags = sorted(tag_cers.items(), key=lambda x: np.median(x[1]))
    tags = [t[0] for t in sorted_tags]
    
    # 1. CERç®±ã²ã’å›³
    cer_data = [tag_cers[tag] for tag in tags]
    bp1 = ax1.boxplot(cer_data, tick_labels=tags, patch_artist=True, notch=True,
                      boxprops=dict(facecolor='lightblue', alpha=0.7),
                      medianprops=dict(color='red', linewidth=2))
    
    ax1.axhline(y=threshold, color='orange', linestyle='--', linewidth=2, 
                label=f'CERé–¾å€¤={threshold}')
    ax1.set_ylabel('CER', fontsize=10, fontweight='bold')
    ax1.set_title('ã‚¿ã‚°åˆ¥CERåˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax1.set_ylim(0, 1.0)
    ax1.grid(axis='y', alpha=0.3)
    ax1.legend(fontsize=9)
    ax1.tick_params(axis='x', rotation=45)
    
    # 2. ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ç®±ã²ã’å›³
    distance_data = [tag_distances[tag] for tag in tags]
    bp2 = ax2.boxplot(distance_data, tick_labels=tags, patch_artist=True, notch=True,
                      boxprops=dict(facecolor='lightcoral', alpha=0.7),
                      medianprops=dict(color='blue', linewidth=2))
    
    ax2.axhline(y=EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2,
                label=f'çµŒé¨“95% (={EMPIRICAL_THRESHOLD_D2:.1f})')
    ax2.axhline(y=WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2,
                label=f'æº–ç•°å¸¸ (={WARNING_THRESHOLD_D2:.1f})')
    ax2.set_ylabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax2.set_title('ã‚¿ã‚°åˆ¥ç•°å¸¸åº¦åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    ax2.legend(fontsize=9)
    ax2.tick_params(axis='x', rotation=45)
    
    # 3. ã‚¿ã‚°åˆ¥ç•°å¸¸ç‡
    tag_anomaly_rates = {}
    for tag in tags:
        total_samples = len(tag_distances[tag])
        anomaly_samples = sum(1 for d in tag_distances[tag] if d > WARNING_THRESHOLD_D2)
        tag_anomaly_rates[tag] = (anomaly_samples / total_samples) * 100 if total_samples > 0 else 0
    
    ax3.bar(tags, [tag_anomaly_rates[tag] for tag in tags], 
            color='coral', alpha=0.7, edgecolor='black')
    ax3.set_ylabel('ç•°å¸¸ç‡ (%)', fontsize=10, fontweight='bold')
    ax3.set_title('ã‚¿ã‚°åˆ¥ç•°å¸¸æ¤œçŸ¥ç‡', fontsize=12, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)
    ax3.tick_params(axis='x', rotation=45)
    
    # å€¤ã‚’æ£’ã®ä¸Šã«è¡¨ç¤º
    for i, rate in enumerate([tag_anomaly_rates[tag] for tag in tags]):
        ax3.text(i, rate + 1, f'{rate:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 4. ã‚µãƒ³ãƒ—ãƒ«æ•°ã¨å“è³ªå‚¾å‘
    tag_stats = []
    for tag in tags:
        n_samples = len(tag_cers[tag])
        avg_cer = np.mean(tag_cers[tag])
        avg_distance = np.mean(tag_distances[tag])
        tag_stats.append({
            'tag': tag,
            'n_samples': n_samples,
            'avg_cer': avg_cer,
            'avg_distance': avg_distance
        })
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    table_data = []
    for stat in tag_stats:
        table_data.append([
            stat['tag'],
            str(stat['n_samples']),
            f"{stat['avg_cer']:.3f}",
            f"{stat['avg_distance']:.1f}",
            f"{tag_anomaly_rates[stat['tag']]:.1f}%"
        ])
    
    ax4.axis('tight')
    ax4.axis('off')
    table = ax4.table(cellText=table_data,
                      colLabels=['ã‚¿ã‚°', 'ã‚µãƒ³ãƒ—ãƒ«æ•°', 'å¹³å‡CER', 'å¹³å‡DÂ²', 'ç•°å¸¸ç‡'],
                      cellLoc='center',
                      loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1.2, 1.5)
    ax4.set_title('ã‚¿ã‚°åˆ¥çµ±è¨ˆã‚µãƒãƒªãƒ¼', fontsize=12, fontweight='bold', pad=20)
    
    plt.suptitle('ã‚¿ã‚°åˆ¥åˆ†æï¼ˆCER + ç•°å¸¸æ¤œçŸ¥çµ±åˆï¼‰', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    output_path = output_dir / 'ocr_tag_analysis_integrated.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ã‚¿ã‚°åˆ¥åˆ†æä¿å­˜: {output_path}")
    plt.close()


def create_performance_scatter(results, distances_d2, output_dir: pathlib.Path, threshold: float = 0.30):
    """CER vs ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®æ•£å¸ƒå›³ï¼ˆç•°å¸¸æ¤œçŸ¥çµ±åˆç‰ˆï¼‰"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    cers = [r['cer'] for r in results]
    latencies = [r['latency_ms'] for r in results]
    ids = [r['id'] for r in results]
    
    # ç•°å¸¸ãƒ¬ãƒ™ãƒ«åˆ¥ã®è‰²åˆ†ã‘
    colors = []
    for d in distances_d2:
        level, _ = classify_anomaly_level(d)
        if level == 'strong_anomaly':
            colors.append('#e74c3c')  # èµ¤
        elif level == 'weak_anomaly':
            colors.append('#f39c12')  # ã‚ªãƒ¬ãƒ³ã‚¸
        else:
            colors.append('#2ecc71')  # ç·‘
    
    # 1. CER vs ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    scatter1 = ax1.scatter(latencies, cers, c=colors, s=150, alpha=0.7, edgecolors='black', linewidth=1.5)
    
    # ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®IDãƒ©ãƒ™ãƒ«
    for i, (lat, cer, id_) in enumerate(zip(latencies, cers, ids)):
        if distances_d2[i] > WARNING_THRESHOLD_D2:
            ax1.annotate(id_, (lat, cer), fontsize=9, fontweight='bold',
                        ha='center', va='bottom', 
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))
    
    ax1.axhline(y=threshold, color='purple', linestyle='--', linewidth=2, label=f'CERé–¾å€¤={threshold}')
    ax1.set_xlabel('å‡¦ç†æ™‚é–“ (ms)', fontsize=10, fontweight='bold')
    ax1.set_ylabel('CER', fontsize=10, fontweight='bold')
    ax1.set_title('CER vs å‡¦ç†æ™‚é–“ï¼ˆç•°å¸¸æ¤œçŸ¥çµ±åˆï¼‰', fontsize=12, fontweight='bold')
    ax1.set_ylim(0, 1.0)
    ax1.grid(alpha=0.3)
    ax1.legend(fontsize=9)
    
    # 2. ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ vs CER
    scatter2 = ax2.scatter(distances_d2, cers, c=colors, s=150, alpha=0.7, edgecolors='black', linewidth=1.5)
    
    for i, (d, cer, id_) in enumerate(zip(distances_d2, cers, ids)):
        if d > WARNING_THRESHOLD_D2:
            ax2.annotate(id_, (d, cer), fontsize=9, fontweight='bold',
                        ha='center', va='bottom',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))
    
    ax2.axvline(x=EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2)
    ax2.axvline(x=WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2)
    ax2.axhline(y=threshold, color='purple', linestyle='--', linewidth=2)
    ax2.set_xlabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax2.set_ylabel('CER', fontsize=10, fontweight='bold')
    ax2.set_title('ç•°å¸¸åº¦ vs CER', fontsize=12, fontweight='bold')
    ax2.set_ylim(0, 1.0)
    ax2.grid(alpha=0.3)
    
    # 3. ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ vs ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    scatter3 = ax3.scatter(distances_d2, latencies, c=colors, s=150, alpha=0.7, edgecolors='black', linewidth=1.5)
    
    ax3.axvline(x=EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2)
    ax3.axvline(x=WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2)
    ax3.set_xlabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax3.set_ylabel('å‡¦ç†æ™‚é–“ (ms)', fontsize=10, fontweight='bold')
    ax3.set_title('ç•°å¸¸åº¦ vs å‡¦ç†æ™‚é–“', fontsize=12, fontweight='bold')
    ax3.grid(alpha=0.3)
    
    # 4. é‹ç”¨æ±ºå®šãƒãƒˆãƒªãƒƒã‚¯ã‚¹
    decision_counts = {'normal': 0, 'warn': 0, 'degrade': 0}
    decision_examples = {'normal': [], 'warn': [], 'degrade': []}
    
    for i, d in enumerate(distances_d2):
        level, decision = classify_anomaly_level(d)
        decision_counts[decision] += 1
        if len(decision_examples[decision]) < 3:  # æœ€å¤§3å€‹ã¾ã§
            decision_examples[decision].append(ids[i])
    
    # æ„æ€æ±ºå®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆé¢¨ã®è¡¨ç¤º
    ax4.axis('off')
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    ax4.text(0.5, 0.95, 'é‹ç”¨æ„æ€æ±ºå®šãƒ«ãƒ¼ãƒ«', fontsize=14, fontweight='bold', 
             ha='center', transform=ax4.transAxes)
    
    # ãƒ«ãƒ¼ãƒ«ã®æç”»
    rules_text = f'''
ã€DÂ² â‰¤ {WARNING_THRESHOLD_D2:.0f}ã€‘é€šå¸¸é‹è»¢ ({decision_counts['normal']}ä»¶)
ã€€â†’ å‡¦ç†ç¶šè¡Œã€æ¨™æº–é–¾å€¤é©ç”¨
ã€€ä¾‹: {', '.join(decision_examples['normal'][:3])}

ã€{WARNING_THRESHOLD_D2:.0f} < DÂ² â‰¤ {EMPIRICAL_THRESHOLD_D2:.0f}ã€‘æº–ç•°å¸¸ ({decision_counts['warn']}ä»¶)  
ã€€â†’ è¦è¦³å¯Ÿã€ãƒ­ã‚°è¨˜éŒ²ã€é–¾å€¤ç·©å’Œãªã—
ã€€ä¾‹: {', '.join(decision_examples['warn'][:3])}

ã€DÂ² > {EMPIRICAL_THRESHOLD_D2:.0f}ã€‘å¼·ã„ç•°å¸¸ ({decision_counts['degrade']}ä»¶)
ã€€â†’ å‰å‡¦ç†å¼·åŒ–â†’å†è©¦è¡Œâ†’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
ã€€ä¾‹: {', '.join(decision_examples['degrade'][:3])}
    '''
    
    ax4.text(0.05, 0.8, rules_text, fontsize=11, ha='left', va='top',
             transform=ax4.transAxes, 
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.3))
    
    # å…¨ä½“å‡¡ä¾‹ï¼ˆå³ä¸‹ã«é…ç½®ï¼‰
    normal_patch = mpatches.Patch(color='#2ecc71', alpha=0.7, label='é€šå¸¸é‹è»¢')
    warn_patch = mpatches.Patch(color='#f39c12', alpha=0.7, label='æº–ç•°å¸¸ï¼ˆè¦è¦³å¯Ÿï¼‰')
    strong_patch = mpatches.Patch(color='#e74c3c', alpha=0.7, label='å¼·ã„ç•°å¸¸ï¼ˆãƒ‡ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰')
    
    fig.legend(handles=[normal_patch, warn_patch, strong_patch], 
               loc='lower right', fontsize=11, bbox_to_anchor=(0.98, 0.02))
    
    plt.suptitle('ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æï¼ˆç•°å¸¸æ¤œçŸ¥çµ±åˆç‰ˆï¼‰', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    output_path = output_dir / 'ocr_performance_integrated.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æä¿å­˜: {output_path}")
    plt.close()
    print(f"âœ… æ•£å¸ƒå›³ä¿å­˜: {output_path}")
    plt.close()


def create_summary_dashboard(results, distances_d2, output_dir: pathlib.Path, threshold: float = 0.30):
    """ç·åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆç•°å¸¸æ¤œçŸ¥çµ±åˆç‰ˆï¼‰"""
    fig = plt.figure(figsize=(20, 14))
    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    ids = [r['id'] for r in results]
    cers = [r['cer'] for r in results]
    quality_ok = [r['quality_ok'] for r in results]
    
    # ç•°å¸¸ãƒ¬ãƒ™ãƒ«åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
    decision_counts = {'normal': 0, 'warn': 0, 'degrade': 0}
    colors = []
    for d in distances_d2:
        level, decision = classify_anomaly_level(d)
        decision_counts[decision] += 1
        if level == 'strong_anomaly':
            colors.append('#e74c3c')
        elif level == 'weak_anomaly':
            colors.append('#f39c12')
        else:
            colors.append('#2ecc71')
    
    # 1. CERæ£’ã‚°ãƒ©ãƒ•ï¼ˆä¸Šæ®µå…¨ä½“ï¼‰
    ax1 = fig.add_subplot(gs[0, :])
    bars = ax1.bar(ids, cers, color=colors, alpha=0.7, edgecolor='black', linewidth=1.2)
    ax1.axhline(y=threshold, color='purple', linestyle='--', linewidth=2, 
                label=f'CERé–¾å€¤={threshold}')
    ax1.set_xlabel('ç”»åƒID', fontsize=12, fontweight='bold')
    ax1.set_ylabel('CER', fontsize=12, fontweight='bold')
    ax1.set_title('OCRç²¾åº¦è©•ä¾¡çµæœï¼ˆç•°å¸¸æ¤œçŸ¥çµ±åˆï¼‰', fontsize=14, fontweight='bold')
    ax1.set_ylim(0, 1.0)
    ax1.grid(axis='y', alpha=0.3)
    ax1.legend(fontsize=10)
    
    for bar, cer in zip(bars, cers):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{cer:.2f}', ha='center', va='bottom', fontsize=8)
    
    # 2. ã‚¨ãƒ³ã‚¸ãƒ³åˆ†å¸ƒ
    ax2 = fig.add_subplot(gs[1, 0])
    engines = [r['engine'] for r in results]
    engine_counts = Counter(engines)
    ax2.pie(engine_counts.values(), labels=engine_counts.keys(), autopct='%1.1f%%',
            colors=['#3498db', '#e67e22', '#95a5a6'], startangle=90)
    ax2.set_title('ä½¿ç”¨ã‚¨ãƒ³ã‚¸ãƒ³åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    
    # 3. åˆæ ¼ç‡ã¨ç•°å¸¸æ¤œçŸ¥ç‡
    ax3 = fig.add_subplot(gs[1, 1])
    passed = sum(quality_ok)
    total = len(results)
    failed = total - passed
    
    quality_data = [passed, failed]
    quality_labels = [f'åˆæ ¼\n({passed}ä»¶)', f'ä¸åˆæ ¼\n({failed}ä»¶)']
    ax3.pie(quality_data, labels=quality_labels, autopct='%1.1f%%',
            colors=['#2ecc71', '#e74c3c'], startangle=90)
    ax3.set_title(f'å“è³ªè©•ä¾¡çµæœ\nåˆæ ¼ç‡: {passed/total*100:.1f}%', 
                  fontsize=12, fontweight='bold')
    
    # 4. ç•°å¸¸æ¤œçŸ¥åˆ†å¸ƒ
    ax4 = fig.add_subplot(gs[1, 2])
    anomaly_data = [decision_counts['normal'], decision_counts['warn'], decision_counts['degrade']]
    anomaly_labels = [f'é€šå¸¸\n({decision_counts["normal"]}ä»¶)', 
                      f'æº–ç•°å¸¸\n({decision_counts["warn"]}ä»¶)',
                      f'å¼·ç•°å¸¸\n({decision_counts["degrade"]}ä»¶)']
    ax4.pie(anomaly_data, labels=anomaly_labels, autopct='%1.1f%%',
            colors=['#2ecc71', '#f39c12', '#e74c3c'], startangle=90)
    ax4.set_title('ç•°å¸¸æ¤œçŸ¥åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    
    # 5. ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    ax5 = fig.add_subplot(gs[2, 0])
    ax5.hist(distances_d2, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
    ax5.axvline(EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2,
                label=f'çµŒé¨“95% (={EMPIRICAL_THRESHOLD_D2:.1f})')
    ax5.axvline(WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2,
                label=f'æº–ç•°å¸¸ (={WARNING_THRESHOLD_D2:.1f})')
    ax5.set_xlabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax5.set_ylabel('é »åº¦', fontsize=10, fontweight='bold')
    ax5.set_title('è·é›¢åˆ†å¸ƒã¨é–¾å€¤', fontsize=12, fontweight='bold')
    ax5.legend(fontsize=9)
    ax5.grid(alpha=0.3)
    
    # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘
    ax6 = fig.add_subplot(gs[2, 1])
    latencies = [r['latency_ms'] for r in results]
    ax6.scatter(distances_d2, latencies, c=colors, s=80, alpha=0.7, edgecolors='black')
    ax6.axvline(EMPIRICAL_THRESHOLD_D2, color='orange', linestyle='-', linewidth=2)
    ax6.axvline(WARNING_THRESHOLD_D2, color='yellow', linestyle=':', linewidth=2)
    ax6.set_xlabel('ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ DÂ²', fontsize=10, fontweight='bold')
    ax6.set_ylabel('å‡¦ç†æ™‚é–“ (ms)', fontsize=10, fontweight='bold')
    ax6.set_title('ç•°å¸¸åº¦ vs å‡¦ç†æ™‚é–“', fontsize=12, fontweight='bold')
    ax6.grid(alpha=0.3)
    
    # 7. çµ±è¨ˆã‚µãƒãƒªãƒ¼
    ax7 = fig.add_subplot(gs[2, 2])
    ax7.axis('off')
    
    # çµ±è¨ˆè¨ˆç®—
    avg_cer = np.mean(cers)
    avg_latency = np.mean(latencies)
    avg_distance = np.mean(distances_d2)
    anomaly_rate = (decision_counts['warn'] + decision_counts['degrade']) / total * 100
    
    stats_text = f'''
ã€ç·åˆçµ±è¨ˆã€‘
â€¢ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total}ä»¶
â€¢ å¹³å‡CER: {avg_cer:.3f}
â€¢ å¹³å‡å‡¦ç†æ™‚é–“: {avg_latency:.1f}ms
â€¢ å¹³å‡ç•°å¸¸åº¦: {avg_distance:.1f}DÂ²

ã€å“è³ªè©•ä¾¡ã€‘
â€¢ åˆæ ¼ç‡: {passed/total*100:.1f}% ({passed}/{total})
â€¢ ç•°å¸¸æ¤œçŸ¥ç‡: {anomaly_rate:.1f}%

ã€é–¾å€¤è¨­å®šã€‘
â€¢ ç†è«–95%: {THEORETICAL_THRESHOLD_D2:.1f}DÂ²
â€¢ çµŒé¨“95%: {EMPIRICAL_THRESHOLD_D2:.1f}DÂ²
â€¢ æº–ç•°å¸¸: {WARNING_THRESHOLD_D2:.1f}DÂ²

ã€é‹ç”¨ãƒ«ãƒ¼ãƒ«ã€‘
â€¢ DÂ² â‰¤ {WARNING_THRESHOLD_D2:.0f}: é€šå¸¸é‹è»¢
â€¢ {WARNING_THRESHOLD_D2:.0f} < DÂ² â‰¤ {EMPIRICAL_THRESHOLD_D2:.0f}: è¦è¦³å¯Ÿ
â€¢ DÂ² > {EMPIRICAL_THRESHOLD_D2:.0f}: ãƒ‡ã‚°ãƒ¬ãƒ¼ãƒ‰å¯¾å¿œ
    '''
    
    ax7.text(0.05, 0.95, stats_text, fontsize=11, ha='left', va='top',
             transform=ax7.transAxes,
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))
    
    # å…¨ä½“ã‚¿ã‚¤ãƒˆãƒ«
    fig.suptitle('OCRå“è³ªç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - ç•°å¸¸æ¤œçŸ¥çµ±åˆç‰ˆ', 
                 fontsize=18, fontweight='bold', y=0.98)
    
    output_path = output_dir / 'ocr_integrated_dashboard.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä¿å­˜: {output_path}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(description='OCRãƒ†ã‚¹ãƒˆçµæœã®å¯è¦–åŒ– + ç•°å¸¸æ¤œçŸ¥åˆ†æ')
    parser.add_argument('--input', type=str, default='tests/outputs/ocr_dataset_eval.json',
                        help='å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output', type=str, default='tests/outputs',
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--threshold', type=float, default=0.30,
                        help='CERé–¾å€¤')
    parser.add_argument('--cov_estimator', type=str, default='ledoit_wolf',
                        choices=['ledoit_wolf', 'empirical'],
                        help='å…±åˆ†æ•£æ¨å®šæ‰‹æ³•')
    parser.add_argument('--exclude_stress', action='store_true',
                        help='ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆç”»åƒï¼ˆ012ãªã©ï¼‰ã‚’é™¤å¤–')
    args = parser.parse_args()
    
    # ãƒ‘ã‚¹è¨­å®š
    input_path = pathlib.Path(args.input)
    output_dir = pathlib.Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if not input_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {input_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {input_path}")
    all_results = load_results(input_path)
    
    # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆé™¤å¤–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    if args.exclude_stress:
        # 012ã®ã‚ˆã†ãªæ„å›³çš„å›°é›£ç”»åƒã‚’é™¤å¤–
        results = [r for r in all_results if not any(tag in r['tags'] for tag in ['012', 'stress', 'impossible'])]
        excluded_count = len(all_results) - len(results)
        print(f"   ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆé™¤å¤–: {excluded_count}ä»¶ â†’ åˆ†æå¯¾è±¡: {len(results)}ä»¶")
    else:
        results = all_results
        print(f"   {len(results)}ä»¶ã®ãƒ†ã‚¹ãƒˆçµæœï¼ˆã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå«ã‚€ï¼‰")
    
    if len(results) < 5:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã™ãã¾ã™ï¼ˆæœ€ä½5ä»¶å¿…è¦ï¼‰")
        sys.exit(1)
    
    # ç‰¹å¾´é‡æŠ½å‡ºã¨ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢è¨ˆç®—
    print("\nğŸ” ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢è¨ˆç®—ä¸­...")
    features = extract_features(results)
    distances_d2, cov_version = compute_mahalanobis_distances(features, args.cov_estimator)
    
    print(f"   ç‰¹å¾´é‡: {features.shape}")
    print(f"   å…±åˆ†æ•£æ¨å®š: {cov_version}")
    print(f"   è·é›¢ç¯„å›²: {distances_d2.min():.1f} - {distances_d2.max():.1f} DÂ²")
    
    # ç•°å¸¸æ¤œçŸ¥ã‚µãƒãƒªãƒ¼
    anomaly_counts = {'normal': 0, 'warn': 0, 'strong': 0}
    for d in distances_d2:
        level, _ = classify_anomaly_level(d)
        if level == 'strong_anomaly':
            anomaly_counts['strong'] += 1
        elif level == 'weak_anomaly':
            anomaly_counts['warn'] += 1
        else:
            anomaly_counts['normal'] += 1
    
    print(f"   ç•°å¸¸æ¤œçŸ¥çµæœ:")
    print(f"     é€šå¸¸: {anomaly_counts['normal']}ä»¶")
    print(f"     æº–ç•°å¸¸: {anomaly_counts['warn']}ä»¶ (è¦è¦³å¯Ÿ)")
    print(f"     å¼·ç•°å¸¸: {anomaly_counts['strong']}ä»¶ (ãƒ‡ã‚°ãƒ¬ãƒ¼ãƒ‰å¯¾å¿œ)")
    
    # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
    print("\nğŸ“ˆ ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")
    create_cer_bar_chart(results, distances_d2, output_dir, args.threshold)
    create_mahalanobis_analysis(results, distances_d2, output_dir)
    create_tag_analysis(results, distances_d2, output_dir, args.threshold)
    create_performance_scatter(results, distances_d2, output_dir, args.threshold)
    create_summary_dashboard(results, distances_d2, output_dir, args.threshold)
    
    # é‹ç”¨ãƒ­ã‚°å‡ºåŠ›
    print("\nğŸ“ é‹ç”¨ãƒ­ã‚°å‡ºåŠ›ä¸­...")
    log_path = create_operational_log(results, distances_d2, output_dir, cov_version)
    
    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print(f"\nâœ… åˆ†æå®Œäº†ï¼")
    print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"   ğŸ“Š ã‚°ãƒ©ãƒ•: 5ç¨®é¡ç”Ÿæˆ")
    print(f"   ğŸ“ é‹ç”¨ãƒ­ã‚°: {log_path.name}")
    print(f"\nğŸ¯ é‹ç”¨æ¨å¥¨äº‹é …:")
    print(f"   â€¢ çµŒé¨“é–¾å€¤ {EMPIRICAL_THRESHOLD_D2:.0f}DÂ² ã‚’ç•°å¸¸åˆ¤å®šåŸºæº–ã¨ã—ã¦æ¡ç”¨")
    print(f"   â€¢ æº–ç•°å¸¸ {WARNING_THRESHOLD_D2:.0f}DÂ² è¶…éæ™‚ã¯è¦è¦³å¯Ÿ")
    print(f"   â€¢ å¼·ç•°å¸¸ã¯å‰å‡¦ç†å¼·åŒ–â†’å†è©¦è¡Œâ†’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    if args.exclude_stress:
        print(f"   â€¢ ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã¯åˆ¥æ ã§ç®¡ç†ï¼ˆå›å¸°ãƒ†ã‚¹ãƒˆã‹ã‚‰åˆ†é›¢ï¼‰")


if __name__ == '__main__':
    main()
